# Tsumugi Spark

![](static/tsumugi-spark-logo.png)

**_NOTE:_** _Tsumugi Shiraui is a chimera: a hybrid of Human and Gauna. She combines the chaotic power of Gauna with a Human intillegence and empathia. Like an original character of the Manga "Knights of Sidonia", this project aims to make a hybrid of very powerful but hard to learn and use Deequ Scala Library with a usability and simplicity of Spark Connect (PySpark Connect, Spark Connect Go, Spark Connect Rust, etc.)._

## About

The goal of the project is to create a modern, SparkConnect native wrapper on top of a beautiful and very fast Data Quality library [AWS Deequ](https://github.com/awslabs/deequ).

## Why another wrapper?

While Amazon Deequ itself is very well maintained, the existing PyDeequ wrapper has two main problems:

1. It is made via direct `py4j` calls to underlying Deequ Scala library. The first problem here is that it cannot be used in any SparkConnect environment. The bigger problem is that `py4j` is not so well suited for working with Scala code: just try to create `Option[Long]` from Python with `py4j` to understand what I'm talking about;
2. It is suffering from the lack of maintenance (I think that the strong reason of it is the p.1). Just check [this issue](https://github.com/awslabs/python-deequ/issues/192).

## Goals of the project

- Maintain `proto3` definitions of basic Deequ Scala structures (like Check, Analyzer, AnoalyDetectionStrategy, VerificationSuite, Constraint, etc.);
- Maintain Scala SparkConnect Plugin that allows working with Deequ fro any client;
- (Low Priority) Maintain Python client that provides a user-friendly API on top of generated by `protoc` classes

## Non-goals of the project

- Creating just another low-code / zero-code Data Quality tool: Deequ Scala is a very cool, Apache Spark Native, Data Quality engine that allows to compute a lot of things on scale. Anyone can create own `yaml` / `json` parametrization, dahsboards, or any other drag-n-drop low-code solution using Deequ.

## Project structure

### Protobuf messages

`tsumugi-server/src/main/protobuf/` contains messages that define the main structures of Deequ Scala library:

- `VerificationSuite` as a top-level Deequ object. See `suite.proto` for details;
- `Analyzer` object that is defined using `oneof` from list of analyzers (`CountDistinct`, `Size`, `Compliance`, etc.). See `analyzers.proto` for details of the implementation;
- `AnaomalyDetection` and strategies of anomaly detection. See `strategies.proto` for details;
- `Check` that is defined using `Constraint`, `CheckLevel` and description;
- `Constraint` itself that is defined as an Analyzer (that computes a metric), a reference value and a comparison sign;

### SparkConnect Plugin

`tsumugi-server/src/main/scala/org/apache/spark/sql/DeequConnectPlugin.scala` contains the plugin itself code. It is made very simple, about 50 lines of code. It just checks if the message is `VerificationSuite`, passes it into `DeequSuiteBuilder` and after that packs it back to `Relation`.

### Deequ Suite Builder

`tsumugi-server/src/main/scala/com/ssinchenko/DeequSuiteBuilder.scala` contains a code, that creates Deequ objects from protobuf messages. It maps enums and constants into enums and constants of Deequ, creates `com.amazon.deequ` objects from the corresponding protobuf messages. Returns a ready to use Deequ top-level structure.


## Getting Started

At the moment there is no CI/CD, release, or python package. To run an example to the following (assumed POSIX system):

- Build Tsumugi Server Plugin:
    - `cd tsumugi-server`
    - `mvn clean package -DskipTests`
- Download and unpack Apache Spark 3.5.1:
    - `wget https://www.apache.org/dyn/closer.lua/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz`
    - `tar -xaf tar -xvf spark-3.5.1-bin-hadoop3.tgz`
- Go to the unpacked spark folder and do the following:
    - Copy tsumugi jar here: `cp ../tsumugi-server/target/tsumugi-server-1.0-SNAPSHOT.jar ./`
    - Download Deequ jar: `wget https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.7-spark-3.5/deequ-2.0.7-spark-3.5.jar`
    - Download Protobuf Java Runtime: `wget https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.25.1/protobuf-java-3.25.1.jar`
- In the spark folder create a bash script `run-connect.sh` with the following code:

```sh
./sbin/start-connect-server.sh \
  --wait \
  --verbose \
  --jars tsumugi-server-1.0-SNAPSHOT.jar,protobuf-java-3.25.1.jar,deequ-2.0.7-spark-3.5.jar \
  --conf spark.connect.extensions.relation.classes=org.apache.spark.sql.DeequConnectPlugin \
  --packages org.apache.spark:spark-connect_2.12:3.5.1
```

- Run `sh run-connect.sh` that will start a SparkConnect Server with Tsumugi plugin and Deequ library in the ClassPath.
- Go back to the root of the project: `cd ..`
- Install `pyspark[connect]==3.5.1`:
    - `python3.10 -m venv .venv`
    - `source .venv/bin/activate`
    - `pip install pyspark[connect]==3.5.1`
- Run `base_example.py`: `python tsumugi_python/examples/base_example.py` and enjoy it!

_**NOTE:**_ _The short guide above assumed a lot of things already installed, like Java 11, Maven, protoc, buf, Python 3.10, etc. I will create a more advanced documentation soon, after merging an initial RP._
